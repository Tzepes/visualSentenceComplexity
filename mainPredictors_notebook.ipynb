{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import textstat\n",
    "import seaborn as sns\n",
    "from enum import Enum\n",
    "\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "from nltk.corpus import stopwords, wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk import pos_tag, Tree\n",
    "from nltk.corpus import brown\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import xgboost as xg\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "from corpus_data.colors_corpus import color_words\n",
    "from corpus_data.wordsOfNums_corpus import words_of_numbers\n",
    "from corpus_data.prepositions_corpus import prepositions\n",
    "from corpus_data.positioning_corpus import position_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('punkt_tab')\n",
    "# nltk.download(\"stopwords\")\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('brown')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "brown_words = brown.words()\n",
    "brown_freq_dist = nltk.FreqDist(brown_words)\n",
    "total_brown_words = len(brown_words)\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframes and arrays of columns of raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"./data/train.csv\")\n",
    "test_df = pd.read_csv(\"./data/test.csv\")\n",
    "val_df = pd.read_csv(\"./data/val.csv\")\n",
    "\n",
    "train_ids = train_df['id'].tolist()\n",
    "train_sentences = train_df['text'].tolist()\n",
    "train_scores = train_df['score'].tolist()\n",
    "\n",
    "test_ids = test_df['id'].tolist()\n",
    "test_sentences = test_df['text'].tolist()\n",
    "\n",
    "valid_ids = val_df['id'].tolist()\n",
    "valid_sentences = val_df['text'].tolist()\n",
    "valid_scores = val_df['score'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions for preprocessing and vectorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(words_token):\n",
    "    for word in words_token:\n",
    "        if word in stop_words:\n",
    "            words_token.remove(word)\n",
    "    return words_token\n",
    "\n",
    "def lemmetizer(words_token):\n",
    "    words_lemm = []\n",
    "    for word in words_token:\n",
    "        words_lemm.append(lemmatizer.lemmatize(word))\n",
    "    return words_lemm\n",
    "\n",
    "def sentence_preprocessing(sentence, lemmetizing=True):\n",
    "    sentence = sentence.lower()\n",
    "    sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
    "\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    words = remove_stopwords(words)\n",
    "\n",
    "    if lemmetizing: \n",
    "        words = lemmetizer(words)\n",
    "        return \" \".join(words)\n",
    "    else:\n",
    "        return words\n",
    "\n",
    "def sentence_to_vec(sentences, model):\n",
    "    sentences_vectors = []\n",
    "    for sentence in sentences:\n",
    "        word_vectors = [model.wv[word] for word in sentence if word in model.wv]\n",
    "        if len(word_vectors) > 0:\n",
    "            sentence_vector = np.mean(word_vectors, axis=0)\n",
    "        else:\n",
    "            sentence_vector = np.zeros(model.vector_size)\n",
    "        sentences_vectors.append(sentence_vector)\n",
    "    return np.array(sentences_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asignment of pre-processed sentences to lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences_clean = [sentence_preprocessing(sentence) for sentence in train_sentences]\n",
    "test_sentences_clean = [sentence_preprocessing(sentence) for sentence in test_sentences]\n",
    "validation_sentences_clean = [sentence_preprocessing(sentence) for sentence in valid_sentences]\n",
    "\n",
    "w2v_trainSentences_clean = [sentence_preprocessing(sentence, lemmetizing=False) for sentence in train_sentences]\n",
    "w2v_testSentences_clean = [sentence_preprocessing(sentence, lemmetizing=False) for sentence in test_sentences]\n",
    "w2v_valSentences_clean = [sentence_preprocessing(sentence, lemmetizing=False) for sentence in valid_sentences]\n",
    "\n",
    "train_ids_df = pd.DataFrame(train_ids, columns=['train_ids'])\n",
    "train_sentences_df = pd.DataFrame(train_sentences_clean, columns=['train_sentences'])\n",
    "train_scores_df = pd.DataFrame(train_scores, columns=['train_scores'])\n",
    "\n",
    "valid_ids_df = pd.DataFrame(valid_ids, columns=['valid_ids'])\n",
    "valid_sentences_df = pd.DataFrame(validation_sentences_clean, columns=['valid_sentences'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CharacterCount(sentences):\n",
    "    character_count = []\n",
    "    for sentence in sentences:\n",
    "        removed_punctuation = re.sub(r'[^\\w\\s]', '', sentence)\n",
    "        removed_spaces = re.sub(r'\\s', '', removed_punctuation)\n",
    "        character_count.append(len(removed_spaces))\n",
    "    return np.array(character_count).reshape(-1, 1)\n",
    "\n",
    "def WordRarity(sentences):\n",
    "    word_rarity = []\n",
    "    for sentence in sentences:\n",
    "        sentence_rarity = []\n",
    "        for word in word_tokenize(sentence):\n",
    "            if word.isalpha() and word.lower() not in stop_words:\n",
    "                sentence_rarity.append(-np.log((brown_freq_dist[word.lower()] + 1) / total_brown_words))\n",
    "        word_rarity.append(np.mean(sentence_rarity) if sentence_rarity else 0)\n",
    "    return np.array(word_rarity).reshape(-1, 1)\n",
    "\n",
    "class PoS(Enum):\n",
    "    NOUNS = \"nouns\"\n",
    "    ADJECTIVES = \"adjectives\"\n",
    "    VERBS = \"verbs\"\n",
    "    ADVERBS = \"adverbs\"\n",
    "\n",
    "def PartOfSpeech_Labels(sentences, pos_tags):\n",
    "    pos_count = []\n",
    "    for sentence in sentences:\n",
    "        count = 0\n",
    "        for word in word_tokenize(sentence):\n",
    "            match pos_tags:\n",
    "                case PoS.NOUNS:\n",
    "                    if any(synset.pos() == 'n' for synset in wn.synsets(word)):\n",
    "                        count += 1\n",
    "                case PoS.ADJECTIVES:\n",
    "                    if any(synset.pos() == 'a' for synset in wn.synsets(word)):\n",
    "                        count += 1\n",
    "                case PoS.VERBS:\n",
    "                    if any(synset.pos() == 'v' for synset in wn.synsets(word)):\n",
    "                        count += 1\n",
    "                case PoS.ADVERBS:\n",
    "                    if any(synset.pos() == 'r' for synset in wn.synsets(word)):\n",
    "                        count += 1\n",
    "        pos_count.append(count)\n",
    "    return np.array(pos_count).reshape(-1, 1)\n",
    "\n",
    "def ColorCount(sentences):\n",
    "    colors_count = []\n",
    "    for sentence in sentences:\n",
    "        count = 0\n",
    "        for word in word_tokenize(sentence):\n",
    "            if word in color_words:\n",
    "                count += 1\n",
    "        colors_count.append(count)\n",
    "    return np.array(colors_count).reshape(-1, 1)\n",
    "\n",
    "def WordLengthRatio(sentences):\n",
    "    word_length_ratio = []\n",
    "    for sentence in sentences:\n",
    "        word_length_ratio.append(textstat.avg_letter_per_word(sentence))\n",
    "    return np.array(word_length_ratio).reshape(-1, 1)\n",
    "\n",
    "def PrepositionsCount(sentences):\n",
    "    prepositions_count = []\n",
    "    for sentence in sentences:\n",
    "        count = 0\n",
    "        for word in word_tokenize(sentence):\n",
    "            if word in prepositions:\n",
    "                count += 1\n",
    "        prepositions_count.append(count)\n",
    "    return np.array(prepositions_count).reshape(-1, 1)\n",
    "\n",
    "def DescriptivePositioning(sentences):\n",
    "    position_count = []\n",
    "    for sentence in sentences:\n",
    "        count = 0\n",
    "        for word in word_tokenize(sentence):\n",
    "            if word in position_words:\n",
    "                count += 1\n",
    "        position_count.append(count)\n",
    "    return np.array(position_count).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ExtractFeatures(sentences):\n",
    "    features = {\n",
    "        'word_rarity': WordRarity(sentences),\n",
    "        'adjective_count': PartOfSpeech_Labels(sentences, PoS.ADJECTIVES),\n",
    "        'adverb_count': PartOfSpeech_Labels(sentences, PoS.ADVERBS),\n",
    "        'color_count': ColorCount(sentences),\n",
    "        'word_length_ratio': WordLengthRatio(sentences),\n",
    "        'descriptive_positioning': DescriptivePositioning(sentences)\n",
    "    }\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment of features lists to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = ExtractFeatures(train_sentences_clean)\n",
    "test_features = ExtractFeatures(test_sentences_clean)\n",
    "valid_features = ExtractFeatures(validation_sentences_clean)\n",
    "\n",
    "X_train_features = np.hstack([train_features[feature] for feature in train_features])\n",
    "X_test_features = np.hstack([test_features[feature] for feature in test_features])\n",
    "X_valid_features = np.hstack([valid_features[feature] for feature in valid_features])\n",
    "\n",
    "train_features_df = pd.DataFrame(X_train_features, columns=train_features.keys())\n",
    "test_features_df = pd.DataFrame(X_test_features, columns=test_features.keys())\n",
    "valid_features_df = pd.DataFrame(X_valid_features, columns=valid_features.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_df['sentence'] = train_sentences_clean\n",
    "train_features_df['sentence_id'] = train_ids\n",
    "\n",
    "# test_features_df['sentence'] = test_sentences_clean\n",
    "# test_features_df['sentence_id'] = test_ids\n",
    "\n",
    "# valid_features_df['sentence'] = validation_sentences_clean\n",
    "# valid_features_df['sentence_id'] = valid_ids\n",
    "\n",
    "print(\"Training Features Table:\")\n",
    "print(train_features_df.head())\n",
    "\n",
    "# print(\"\\nValidation Features Table:\")\n",
    "# print(valid_features_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot features with pairplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exclude non-numeric columns for correlation matrix\n",
    "numeric_train_features_df = train_features_df.select_dtypes(include=[np.number])\n",
    "\n",
    "plotting_of_featuresDF = pd.concat([numeric_train_features_df, train_scores_df], axis = 1)\n",
    "axis = sns.pairplot(data = plotting_of_featuresDF, plot_kws = dict(color = \"maroon\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot features for correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_train_features_df['train_scores'] = train_scores\n",
    "\n",
    "featureCorrelation = numeric_train_features_df.corr()\n",
    "sns.heatmap(featureCorrelation, annot=True, cmap='Blues', linewidths=1, annot_kws={\"weight\": \"bold\", \"fontsize\": 10})\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5, min_df=5, stop_words=\"english\")\n",
    "\n",
    "X_train_tfidf = vectorizer.fit_transform(train_sentences)\n",
    "X_test_tfidf = vectorizer.transform(test_sentences)\n",
    "X_valid_tfidf = vectorizer.transform(valid_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine features with vectorized sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Comb_TrainTFIDF = np.hstack([X_train_features, X_train_tfidf.toarray()])\n",
    "X_Comb_TestTFIDF = np.hstack([X_test_features, X_test_tfidf.toarray()])\n",
    "X_Comb_ValidTFIDF = np.hstack([X_valid_features, X_valid_tfidf.toarray()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take and split combined dataframe for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df = pd.DataFrame(X_train_tfidf.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "tfidf_df.to_csv(\"tfidfData.csv\", index=False)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_Comb_TrainTFIDF, train_scores, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient boosted regressor model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GBR_pipeline = Pipeline(steps = [(\"scaler\", scaler), (\"regressor\", GradientBoostingRegressor(n_estimators=500, learning_rate=0.1, max_depth=3, random_state=42, verbose=1))])\n",
    "GBR_pipeline.fit(X_train, y_train)\n",
    "\n",
    "train_score = GBR_pipeline.score(X_train, y_train)\n",
    "test_score = GBR_pipeline.score(X_test, y_test)\n",
    "\n",
    "print(\"Gradient Boosting Regressor Results:\")\n",
    "print(f\"Train score: {train_score}\")\n",
    "print(f\"Test score: {test_score}\")\n",
    "\n",
    "valid_predictions_GBR = GBR_pipeline.predict(X_Comb_ValidTFIDF)\n",
    "spearman_corr, p_value = spearmanr(valid_scores, valid_predictions_GBR)\n",
    "print(f\"Spearman Correlation: {spearman_corr}\")\n",
    "print(f\"P-value: {p_value}\", end=\"\\n\\n\")\n",
    "\n",
    "test_predictions_GBR = GBR_pipeline.predict(X_Comb_TestTFIDF)\n",
    "\n",
    "GBR_predictions_df = pd.DataFrame({\n",
    "    \"id\": test_ids,\n",
    "    \"score\": test_predictions_GBR\n",
    "})\n",
    "GBR_predictions_df.to_csv(\"GBR_predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Ridge model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayesianRidge = linear_model.BayesianRidge(verbose=1)\n",
    "bayesianRidge.fit(X_train, y_train)\n",
    "\n",
    "valid_predictions_bayesian = bayesianRidge.predict(X_Comb_ValidTFIDF)\n",
    "spearman_corr, p_value = spearmanr(valid_scores, valid_predictions_bayesian)\n",
    "print(\"Bayesian Ridge Results:\")\n",
    "print(f\"Spearman Correlation: {spearman_corr}\")\n",
    "print(f\"P-value: {p_value}\", end=\"\\n\\n\")\n",
    "\n",
    "test_predictions_bayesian = bayesianRidge.predict(X_Comb_TestTFIDF)\n",
    "\n",
    "Bayesian_predictions_df = pd.DataFrame({\n",
    "    \"id\": test_ids,\n",
    "    \"score\": test_predictions_bayesian\n",
    "})\n",
    "Bayesian_predictions_df.to_csv(\"bayesianRidge_predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec with XGB Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2vVectorizer = Word2Vec(sentences = w2v_trainSentences_clean, vector_size=100, window=5, min_count=1, workers=4, epochs=100)\n",
    "\n",
    "X_train_w2v = sentence_to_vec(w2v_trainSentences_clean, w2vVectorizer)\n",
    "y_train_w2v = train_scores\n",
    "\n",
    "X_valid_w2v = sentence_to_vec(w2v_valSentences_clean, w2vVectorizer)\n",
    "y_valid_w2v = valid_scores\n",
    "\n",
    "X_test_w2v = sentence_to_vec(w2v_testSentences_clean, w2vVectorizer)\n",
    "\n",
    "X_train_w2v = scaler.fit_transform(X_train_w2v)\n",
    "X_valid_w2v = scaler.transform(X_valid_w2v)\n",
    "X_test_w2v = scaler.transform(X_test_w2v)\n",
    "\n",
    "XGB_Regressor = xg.XGBRegressor(objective='reg:squarederror', n_estimators=500, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "# XGB_Regressor = xg.XGBRegressor(\n",
    "#     objective='reg:squarederror',\n",
    "#     n_estimators=100,\n",
    "#     learning_rate=0.01,\n",
    "#     max_depth=5,\n",
    "#     subsample=0.7,\n",
    "#     colsample_bytree=0.7,\n",
    "#     random_state=42,\n",
    "#     n_jobs=-1\n",
    "# )\n",
    "\n",
    "XGB_Regressor.fit(X_train_w2v, y_train_w2v)\n",
    "\n",
    "valid_predictions_XGB = XGB_Regressor.predict(X_valid_w2v)\n",
    "spearman_corr, p_value = spearmanr(valid_scores, valid_predictions_XGB)\n",
    "\n",
    "print(\"XGBoost Regressor Results:\")\n",
    "print(f\"Spearman Correlation: {spearman_corr}\")\n",
    "print(f\"P-value: {p_value}\")\n",
    "\n",
    "test_predictions_XGB = XGB_Regressor.predict(X_test_w2v)\n",
    "\n",
    "XGB_predictions_df = pd.DataFrame({\n",
    "    \"id\": test_ids,\n",
    "    \"score\": test_predictions_XGB\n",
    "})\n",
    "XGB_predictions_df.to_csv(\"XGB_predictions.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
